{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce CheXNet: Explore Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import other modules and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU count:1\n",
      "0.4.1\n",
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# image imports\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "\n",
    "# data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import random\n",
    "\n",
    "import cxr_dataset as CXR\n",
    "import cxp_dataset as CXP\n",
    "import model_fz as M\n",
    "import eval_model_fz as E\n",
    "\n",
    "#suppress pytorch warnings about source code changes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data import\n",
    "Change CheXNet import on NIH data to import of CheXpert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSV = \"/home/frank_li_zhou/CheXpert-v1.0-small/\"\n",
    "PATH_TO_CSV = \"/home/frank_li_zhou/CheXpert-v1.0/\"\n",
    "\n",
    "PATH_TO_IMAGES = \"/home/frank_li_zhou/\"\n",
    "\n",
    "# use imagenet mean,std for normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# define torchvision transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(224), # changed for pytorch 4.0.1\n",
    "        # because scale doesn't always give 224 x 224, this ensures 224 x\n",
    "        # 224\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# create train/val dataloaders\n",
    "transformed_datasets = {}\n",
    "transformed_datasets['train'] = CXP.CXPDataset(\n",
    "    path_to_images=PATH_TO_IMAGES,\n",
    "    path_to_csv=PATH_TO_CSV,\n",
    "    fold='train',\n",
    "    transform=data_transforms['train'],\n",
    ")\n",
    "transformed_datasets['val'] = CXP.CXPDataset(\n",
    "    path_to_images=PATH_TO_IMAGES,\n",
    "    path_to_csv=PATH_TO_CSV,\n",
    "    fold='val',\n",
    "    transform=data_transforms['val'],\n",
    ")\n",
    "\n",
    "#print(transformed_datasets['train'].df.iloc[1])\n",
    "#print(transformed_datasets['train'][0][0].shape)\n",
    "#print(transformed_datasets['val'][0][0].shape)\n",
    "\n",
    "img = random.choice(transformed_datasets['train'])[0]\n",
    "img = transformed_datasets['train'][0][0]\n",
    "img -= img.min()\n",
    "img /= img.max()\n",
    "\n",
    "print(\"Sample transformed image:\")\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU count:1\n",
      "Running with WD, LR: 0.0001 0.001\n",
      "Uncertain labels are positive\n",
      "Using no uncertain labeling\n",
      "Size of train set: 223414\n",
      "Size of val set: 234\n",
      "Model training start\n",
      "14 May 2019 01:21:26\n",
      "Epoch 1/100\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_CSV = \"/home/frank_li_zhou/CheXpert-v1.0-small/\"\n",
    "PATH_TO_CSV = \"/home/frank_li_zhou/CheXpert-v1.0/\"\n",
    "PATH_TO_IMAGES = \"/home/frank_li_zhou/\"\n",
    "\n",
    "LOGGER = []\n",
    "\n",
    "# adam\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# # sgd\n",
    "# WEIGHT_DECAY = 1e-4\n",
    "# LEARNING_RATE = 0.01\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "preds, aucs = M.train_cnn(PATH_TO_IMAGES, PATH_TO_CSV, LEARNING_RATE, WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%d %b %Y %H:%M:%S\", time.gmtime(time.time()-25200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(CXP)\n",
    "importlib.reload(CXR)\n",
    "importlib.reload(M)\n",
    "importlib.reload(E)\n",
    "\n",
    "PATH_TO_MODEL = \"/home/frank_li_zhou/reproduce-chexnet/pretrained/checkpoint\"\n",
    "PATH_TO_IMAGES = \"/home/frank_li_zhou/\"\n",
    "PATH_TO_CSV = \"/home/frank_li_zhou/CheXpert-v1.0-small/\"\n",
    "\n",
    "def recursion_change_bn(module):\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module.track_running_stats = 1\n",
    "    else:\n",
    "        for i, (name, module1) in enumerate(module._modules.items()):\n",
    "            module1 = recursion_change_bn(module1)\n",
    "    return module\n",
    "\n",
    "checkpoint = torch.load(PATH_TO_MODEL, map_location=lambda storage, loc: storage)\n",
    "model = checkpoint['model']\n",
    "for i, (name, module) in enumerate(model._modules.items()):\n",
    "    module = recursion_change_bn(model)\n",
    "\n",
    "# put model on GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# use imagenet mean,std for normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# define torchvision transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(224), # changed for pytorch 4.0.1\n",
    "        # because scale doesn't always give 224 x 224, this ensures 224 x\n",
    "        # 224\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "preds, metric = E.make_pred_multilabel(data_transforms, model, PATH_TO_IMAGES, PATH_TO_CSV, 'auc')\n",
    "\n",
    "auc = metric.as_matrix(columns=metric.columns[1:])\n",
    "print(auc[~np.isnan(auc)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
